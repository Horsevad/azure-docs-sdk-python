### YamlMime:PythonClass
uid: azure.ai.ml.entities.SparkComponent
name: SparkComponent
fullName: azure.ai.ml.entities.SparkComponent
module: azure.ai.ml.entities
inheritances:
- azure.ai.ml.entities._component.component.Component
- azure.ai.ml.entities._job.parameterized_spark.ParameterizedSpark
- azure.ai.ml.entities._job.spark_job_entry_mixin.SparkJobEntryMixin
- azure.ai.ml.entities._component.code.ComponentCodeMixin
summary: 'Spark component version, used to define a Spark Component or Job.


  ]

  :param outputs: A mapping of output names to output data sources used in the job.

  :type outputs: dict[str, Union[str, ~azure.ai.ml.Output]]

  :param args: The arguments for the job.

  :type args: str'
constructor:
  syntax: 'SparkComponent(*, code: str | PathLike = ''.'', entry: Dict[str, str] |
    SparkJobEntry | None = None, py_files: List[str] | None = None, jars: List[str]
    | None = None, files: List[str] | None = None, archives: List[str] | None = None,
    driver_cores: int | None = None, driver_memory: str | None = None, executor_cores:
    int | None = None, executor_memory: str | None = None, executor_instances: int
    | None = None, dynamic_allocation_enabled: bool | None = None, dynamic_allocation_min_executors:
    int | None = None, dynamic_allocation_max_executors: int | None = None, conf:
    Dict[str, str] | None = None, environment: Environment | str | None = None, inputs:
    Dict | None = None, outputs: Dict | None = None, args: str | None = None, **kwargs)'
  parameters:
  - name: code
    description: 'The source code to run the job. Can be a local path or "http:",
      "https:", or "azureml:" url pointing

      to a remote location. Defaults to ".".'
    isRequired: true
    types:
    - <xref:Union>[<xref:str>, <xref:os.PathLike>]
  - name: entry
    description: The file or class entry point.
    isRequired: true
    types:
    - <xref:Union>[<xref:dict>[<xref:str>, <xref:str>], <xref:azure.ai.ml.entities.SparkJobEntry>]
  - name: py_files
    description: The list of .zip, .egg or .py files to place on the PYTHONPATH for
      Python apps.
    isRequired: true
    types:
    - <xref:list>[<xref:str>]
  - name: jars
    description: The list of .JAR files to include on the driver and executor classpaths.
    isRequired: true
    types:
    - <xref:list>[<xref:str>]
  - name: files
    description: The list of files to be placed in the working directory of each executor.
    isRequired: true
    types:
    - <xref:list>[<xref:str>]
  - name: archives
    description: The list of archives to be extracted into the working directory of
      each executor.
    isRequired: true
    types:
    - <xref:list>[<xref:str>]
  - name: driver_cores
    description: The number of cores to use for the driver process, only in cluster
      mode.
    isRequired: true
    types:
    - <xref:int>
  - name: driver_memory
    description: 'The amount of memory to use for the driver process, formatted as
      strings with a size unit

      suffix ("k", "m", "g" or "t") (e.g. "512m", "2g").'
    isRequired: true
    types:
    - <xref:str>
  - name: executor_cores
    description: The number of cores to use on each executor.
    isRequired: true
    types:
    - <xref:int>
  - name: executor_memory
    description: 'The amount of memory to use per executor process, formatted as strings
      with a size unit

      suffix ("k", "m", "g" or "t") (e.g. "512m", "2g").'
    isRequired: true
    types:
    - <xref:str>
  - name: executor_instances
    description: The initial number of executors.
    isRequired: true
    types:
    - <xref:int>
  - name: dynamic_allocation_enabled
    description: 'Whether to use dynamic resource allocation, which scales the number
      of executors

      registered with this application up and down based on the workload. Defaults
      to False.'
    isRequired: true
    types:
    - <xref:bool>
  - name: dynamic_allocation_min_executors
    description: 'The lower bound for the number of executors if dynamic allocation
      is

      enabled.'
    isRequired: true
    types:
    - <xref:int>
  - name: dynamic_allocation_max_executors
    description: 'The upper bound for the number of executors if dynamic allocation
      is

      enabled.'
    isRequired: true
    types:
    - <xref:int>
  - name: conf
    description: A dictionary with pre-defined Spark configurations key and values.
    isRequired: true
    types:
    - <xref:dict>[<xref:str>, <xref:str>]
  - name: environment
    description: The Azure ML environment to run the job in.
    isRequired: true
    types:
    - <xref:Union>[<xref:str>, <xref:azure.ai.ml.entities.Environment>]
  - name: inputs
    description: A mapping of input names to input data sources used in the job.
    isRequired: true
    types:
    - <xref:dict>[<xref:str>, <xref:Union>[<xref:azure.ai.ml.entities._job.pipeline._io.NodeOutput,azure.ai.ml.Input,str,bool,int,float,Enum,>]
methods:
- uid: azure.ai.ml.entities.SparkComponent.dump
  name: dump
  summary: Dump the component content into a file in yaml format.
  signature: 'dump(dest: str | PathLike | IO, **kwargs) -> None'
  parameters:
  - name: dest
    description: 'The destination to receive this component''s content.

      Must be either a path to a local file, or an already-open file stream.

      If dest is a file path, a new file will be created,

      and an exception is raised if the file exists.

      If dest is an open file, the file will be written to directly,

      and an exception will be raised if the file is not writable.'
    isRequired: true
    types:
    - <xref:Union>[<xref:PathLike>, <xref:str>, <xref:IO>[<xref:AnyStr>]]
attributes:
- uid: azure.ai.ml.entities.SparkComponent.base_path
  name: base_path
  summary: Base path of the resource.
  return:
    description: Base path of the resource
    types:
    - <xref:str>
- uid: azure.ai.ml.entities.SparkComponent.creation_context
  name: creation_context
  summary: Creation context.
  return:
    description: Creation metadata of the resource.
    types:
    - <xref:typing.Optional>[<xref:azure.ai.ml.entities.SystemData>]
- uid: azure.ai.ml.entities.SparkComponent.display_name
  name: display_name
  summary: Display name of the component.
  return:
    description: Display name of the component.
    types:
    - <xref:str>
- uid: azure.ai.ml.entities.SparkComponent.entry
  name: entry
- uid: azure.ai.ml.entities.SparkComponent.environment
  name: environment
- uid: azure.ai.ml.entities.SparkComponent.id
  name: id
  summary: Resource ID.
  return:
    description: Global id of the resource, Azure Resource Manager ID
    types:
    - <xref:typing.Optional>[<xref:str>]
- uid: azure.ai.ml.entities.SparkComponent.inputs
  name: inputs
  summary: Inputs of the component.
  return:
    description: Inputs of the component.
    types:
    - <xref:dict>
- uid: azure.ai.ml.entities.SparkComponent.is_deterministic
  name: is_deterministic
  summary: Whether the component is deterministic.
  return:
    description: Whether the component is deterministic
    types:
    - <xref:bool>
- uid: azure.ai.ml.entities.SparkComponent.outputs
  name: outputs
  summary: Outputs of the component.
  return:
    description: Outputs of the component.
    types:
    - <xref:dict>
- uid: azure.ai.ml.entities.SparkComponent.type
  name: type
  summary: Type of the component, default is 'command'.
  return:
    description: Type of the component.
    types:
    - <xref:str>
- uid: azure.ai.ml.entities.SparkComponent.version
  name: version
  summary: Version of the component.
  return:
    description: Version of the component.
    types:
    - <xref:str>
- uid: azure.ai.ml.entities.SparkComponent.CODE_ID_RE_PATTERN
  name: CODE_ID_RE_PATTERN
  signature: CODE_ID_RE_PATTERN = re.compile('\\/subscriptions\\/(?P<subscription>[\\w,-]+)\\/resourceGroups\\/(?P<resource_group>[\\w,-]+)\\/providers\\/Microsoft\\.MachineLearningServices\\/workspaces\\/(?P<workspace>[\\w,-]+)\\/codes\\/(?P<co)
