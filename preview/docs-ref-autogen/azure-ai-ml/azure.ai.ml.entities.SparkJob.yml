### YamlMime:PythonClass
uid: azure.ai.ml.entities.SparkJob
name: SparkJob
fullName: azure.ai.ml.entities.SparkJob
module: azure.ai.ml.entities
inheritances:
- azure.ai.ml.entities._job.job.Job
- azure.ai.ml.entities._job.parameterized_spark.ParameterizedSpark
- azure.ai.ml.entities._job.job_io_mixin.JobIOMixin
- azure.ai.ml.entities._job.spark_job_entry_mixin.SparkJobEntryMixin
summary: A standalone Spark job.
constructor:
  syntax: 'SparkJob(*, driver_cores: int | None = None, driver_memory: str | None
    = None, executor_cores: int | None = None, executor_memory: str | None = None,
    executor_instances: int | None = None, dynamic_allocation_enabled: bool | None
    = None, dynamic_allocation_min_executors: int | None = None, dynamic_allocation_max_executors:
    int | None = None, inputs: Dict | None = None, outputs: Dict | None = None, compute:
    str | None = None, identity: Dict[str, str] | ManagedIdentityConfiguration | AmlTokenConfiguration
    | UserIdentityConfiguration | None = None, resources: Dict | SparkResourceConfiguration
    | None = None, **kwargs)'
  parameters:
  - name: driver_cores
    description: The number of cores to use for the driver process, only in cluster
      mode.
    isRequired: true
    types:
    - <xref:int>
  - name: driver_memory
    description: 'The amount of memory to use for the driver process, formatted as
      strings with a size unit

      suffix ("k", "m", "g" or "t") (e.g. "512m", "2g").'
    isRequired: true
    types:
    - <xref:str>
  - name: executor_cores
    description: The number of cores to use on each executor.
    isRequired: true
    types:
    - <xref:int>
  - name: executor_memory
    description: 'The amount of memory to use per executor process, formatted as strings
      with a size unit

      suffix ("k", "m", "g" or "t") (e.g. "512m", "2g").'
    isRequired: true
    types:
    - <xref:str>
  - name: executor_instances
    description: The initial number of executors.
    isRequired: true
    types:
    - <xref:int>
  - name: dynamic_allocation_enabled
    description: 'Whether to use dynamic resource allocation, which scales the number
      of executors

      registered with this application up and down based on the workload.'
    isRequired: true
    types:
    - <xref:bool>
  - name: dynamic_allocation_min_executors
    description: 'The lower bound for the number of executors if dynamic allocation
      is

      enabled.'
    isRequired: true
    types:
    - <xref:int>
  - name: dynamic_allocation_max_executors
    description: 'The upper bound for the number of executors if dynamic allocation
      is

      enabled.'
    isRequired: true
    types:
    - <xref:int>
  - name: inputs
    description: The mapping of input data bindings used in the job.
    isRequired: true
    types:
    - <xref:dict>[<xref:str>, <xref:azure.ai.ml.Input>]
  - name: outputs
    description: The mapping of output data bindings used in the job.
    isRequired: true
    types:
    - <xref:dict>[<xref:str>, <xref:azure.ai.ml.Output>]
  - name: compute
    description: The compute resource the job runs on.
    isRequired: true
    types:
    - <xref:str>
  - name: identity
    description: The identity that the Spark job will use while running on compute.
    isRequired: true
    types:
    - <xref:Union>[<xref:dict>[<xref:str>, <xref:str>]<xref:,azure.ai.ml.ManagedIdentityConfiguration,azure.ai.ml.AmlTokenConfiguration,azure.ai.ml.UserIdentityConfiguration>]
  - name: resources
    description: The compute resource configuration for the job.
    isRequired: true
    types:
    - <xref:Union>[<xref:dict>, <xref:azure.ai.ml.entities.SparkResourceConfiguration>]
  - name: experiment_name
    description: The name of the experiment the job will be created under.
    isRequired: true
    types:
    - <xref:str>
  - name: name
    description: The name of the job.
    isRequired: true
    types:
    - <xref:str>
  - name: display_name
    description: The job display name.
    isRequired: true
    types:
    - <xref:str>
  - name: description
    description: The job description.
    isRequired: true
    types:
    - <xref:str>
  - name: tags
    description: The tag dictionary. Tags can be added, removed, and updated.
    isRequired: true
    types:
    - <xref:dict>[<xref:str>, <xref:str>]
  - name: code
    description: 'The source code to run the job. Can be a local path or "http:",
      "https:", or "azureml:" url pointing

      to a remote location.'
    isRequired: true
    types:
    - <xref:Union>[<xref:str>, <xref:os.PathLike>]
  - name: entry
    description: The file or class entry point.
    isRequired: true
    types:
    - <xref:dict>[<xref:str>, <xref:str>]
  - name: py_files
    description: The list of .zip, .egg or .py files to place on the PYTHONPATH for
      Python apps.
    isRequired: true
    types:
    - <xref:list>[<xref:str>]
  - name: jars
    description: The list of .JAR files to include on the driver and executor classpaths.
    isRequired: true
    types:
    - <xref:list>[<xref:str>]
  - name: files
    description: The list of files to be placed in the working directory of each executor.
    isRequired: true
    types:
    - <xref:list>[<xref:str>]
  - name: archives
    description: The list of archives to be extracted into the working directory of
      each executor.
    isRequired: true
    types:
    - <xref:list>[<xref:str>]
  - name: conf
    description: A dictionary with pre-defined Spark configurations key and values.
    isRequired: true
    types:
    - <xref:dict>[<xref:str>, <xref:str>]
  - name: environment
    description: The Azure ML environment to run the job in.
    isRequired: true
    types:
    - <xref:Union>[<xref:str>, <xref:azure.ai.ml.entities.Environment>]
  - name: args
    description: The arguments for the job.
    isRequired: true
    types:
    - <xref:str>
methods:
- uid: azure.ai.ml.entities.SparkJob.dump
  name: dump
  summary: Dump the job content into a file in yaml format.
  signature: 'dump(dest: str | PathLike | IO, **kwargs) -> None'
  parameters:
  - name: dest
    description: 'The destination to receive this job''s content.

      Must be either a path to a local file, or an already-open file stream.

      If dest is a file path, a new file will be created,

      and an exception is raised if the file exists.

      If dest is an open file, the file will be written to directly,

      and an exception will be raised if the file is not writable.'
    isRequired: true
    types:
    - <xref:Union>[<xref:PathLike>, <xref:str>, <xref:IO>[<xref:AnyStr>]]
- uid: azure.ai.ml.entities.SparkJob.filter_conf_fields
  name: filter_conf_fields
  summary: 'Filters out the fields of the conf attribute that are not among the Spark
    configuration fields

    listed in ~azure.ai.ml._schema.job.parameterized_spark.CONF_KEY_MAP and returns
    them in their own dictionary.'
  signature: filter_conf_fields() -> Dict[str, str]
  return:
    description: A dictionary of the conf fields that are not Spark configuration
      fields.
    types:
    - <xref:dict>[<xref:str>, <xref:str>]
attributes:
- uid: azure.ai.ml.entities.SparkJob.base_path
  name: base_path
  summary: Base path of the resource.
  return:
    description: Base path of the resource
    types:
    - <xref:str>
- uid: azure.ai.ml.entities.SparkJob.creation_context
  name: creation_context
  summary: Creation context.
  return:
    description: Creation metadata of the resource.
    types:
    - <xref:typing.Optional>[<xref:azure.ai.ml.entities.SystemData>]
- uid: azure.ai.ml.entities.SparkJob.entry
  name: entry
- uid: azure.ai.ml.entities.SparkJob.environment
  name: environment
- uid: azure.ai.ml.entities.SparkJob.id
  name: id
  summary: Resource ID.
  return:
    description: Global id of the resource, Azure Resource Manager ID
    types:
    - <xref:typing.Optional>[<xref:str>]
- uid: azure.ai.ml.entities.SparkJob.identity
  name: identity
  summary: The identity that the Spark job will use while running on compute.
  return:
    types:
    - <xref:Union>[<xref:azure.ai.ml.ManagedIdentityConfiguration>, <xref:azure.ai.ml.AmlTokenConfiguration,azure.ai.ml.UserIdentityConfiguration>]
- uid: azure.ai.ml.entities.SparkJob.inputs
  name: inputs
- uid: azure.ai.ml.entities.SparkJob.log_files
  name: log_files
  summary: Job output files.
  return:
    description: Dictionary of log names to url.
    types:
    - <xref:Optional>[<xref:Dict>[<xref:str>, <xref:str>]]
- uid: azure.ai.ml.entities.SparkJob.outputs
  name: outputs
- uid: azure.ai.ml.entities.SparkJob.resources
  name: resources
  summary: The compute resource configuration for the job.
  return:
    types:
    - <xref:azure.ai.ml.entities.SparkResourceConfiguration>
- uid: azure.ai.ml.entities.SparkJob.status
  name: status
  summary: "Status of the job.\n\nCommon values returned include \"Running\", \"Completed\"\
    , and \"Failed\".\n\n> [!NOTE]\n> NotStarted - This is a temporary state client-side\
    \ Run objects are in before cloud submission.\n>\n> \n>\n> Starting - The Run\
    \ has started being processed in the cloud. The caller has a run ID at this point.\n\
    >\n> \n>\n> Provisioning - Returned when on-demand compute is being created for\
    \ a given job submission.\n>\n> \n>\n> Preparing - The run environment is being\
    \ prepared:\n>\n> \n>\n> docker image build\n>\n> \n>\n> conda environment setup\n\
    >\n> \n>\n> Queued - The job is queued in the compute target. For example, in\
    \ BatchAI the job is in queued state\n>\n> \n>\n> while waiting for all the requested\
    \ nodes to be ready.\n>\n> \n>\n> Running - The job started to run in the compute\
    \ target.\n>\n> \n>\n> Finalizing - User code has completed and the run is in\
    \ post-processing stages.\n>\n> \n>\n> CancelRequested - Cancellation has been\
    \ requested for the job.\n>\n> \n>\n> Completed - The run completed successfully.\
    \ This includes both the user code and run\n>\n> \n>\n> post-processing stages.\n\
    >\n> \n>\n> Failed - The run failed. Usually the Error property on a run will\
    \ provide details as to why.\n>\n> \n>\n> Canceled - Follows a cancellation request\
    \ and indicates that the run is now successfully cancelled.\n>\n> \n>\n> NotResponding\
    \ - For runs that have Heartbeats enabled, no heartbeat has been recently sent.\n\
    >"
  return:
    description: Status of the job.
    types:
    - <xref:str>
- uid: azure.ai.ml.entities.SparkJob.studio_url
  name: studio_url
  summary: Azure ML studio endpoint.
  return:
    description: URL to the job detail page.
    types:
    - <xref:Optional>[<xref:str>]
- uid: azure.ai.ml.entities.SparkJob.type
  name: type
  summary: Type of the job, supported are 'command' and 'sweep'.
  return:
    description: Type of the job.
    types:
    - <xref:str>
- uid: azure.ai.ml.entities.SparkJob.CODE_ID_RE_PATTERN
  name: CODE_ID_RE_PATTERN
  signature: CODE_ID_RE_PATTERN = re.compile('\\/subscriptions\\/(?P<subscription>[\\w,-]+)\\/resourceGroups\\/(?P<resource_group>[\\w,-]+)\\/providers\\/Microsoft\\.MachineLearningServices\\/workspaces\\/(?P<workspace>[\\w,-]+)\\/codes\\/(?P<co)
