### YamlMime:PythonClass
uid: azure.ai.evaluation.BleuScoreEvaluator
name: BleuScoreEvaluator
fullName: azure.ai.evaluation.BleuScoreEvaluator
module: azure.ai.evaluation
summary: "Evaluator that computes the BLEU Score between two strings.\n\nBLEU (Bilingual\
  \ Evaluation Understudy) score is commonly used in natural language processing (NLP)\
  \ and machine\ntranslation. It is widely used in text summarization and text generation\
  \ use cases. It evaluates how closely the\ngenerated text matches the reference\
  \ text. The BLEU score ranges from 0 to 1, with higher scores indicating\nbetter\
  \ quality.\n\n**Usage**\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"\
  names\": [], \"dupnames\": [], \"backrefs\": [], \"xml:space\": \"preserve\", \"\
  force\": false, \"language\": \"python\", \"highlight_args\": {}, \"linenos\": false}\
  \ -->\n\n````python\n\n   eval_fn = BleuScoreEvaluator()\n   result = eval_fn(\n\
  \       response=\"Tokyo is the capital of Japan.\",\n       ground_truth=\"The\
  \ capital of Japan is Tokyo.\")\n   ````\n\n**Output format**\n\n<!-- literal_block\
  \ {\"ids\": [], \"classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\":\
  \ [], \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
  highlight_args\": {}, \"linenos\": false} -->\n\n````python\n\n   {\n       \"bleu_score\"\
  : 0.22\n   }\n   ````"
constructor:
  syntax: BleuScoreEvaluator()
