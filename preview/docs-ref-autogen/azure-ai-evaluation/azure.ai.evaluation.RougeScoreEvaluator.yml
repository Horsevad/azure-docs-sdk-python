### YamlMime:PythonClass
uid: azure.ai.evaluation.RougeScoreEvaluator
name: RougeScoreEvaluator
fullName: azure.ai.evaluation.RougeScoreEvaluator
module: azure.ai.evaluation
summary: "Evaluator for computes the ROUGE scores between two strings.\n\nROUGE (Recall-Oriented\
  \ Understudy for Gisting Evaluation) is a set of metrics used to evaluate automatic\n\
  summarization and machine translation. It measures the overlap between generated\
  \ text and reference summaries.\nROUGE focuses on recall-oriented measures to assess\
  \ how well the generated text covers the reference text. Text\nsummarization and\
  \ document comparison are among optimal use cases for ROUGE, particularly in scenarios\
  \ where text\ncoherence and relevance are critical.\n\n**Usage**\n\n<!-- literal_block\
  \ {\"ids\": [], \"classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\":\
  \ [], \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
  highlight_args\": {}, \"linenos\": false} -->\n\n````python\n\n   eval_fn = RougeScoreEvaluator(rouge_type=RougeType.ROUGE_1)\n\
  \   result = eval_fn(\n       response=\"Tokyo is the capital of Japan.\",\n   \
  \    ground_truth=\"The capital of Japan is Tokyo.\")\n   ````\n\n**Output format**\n\
  \n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"dupnames\"\
  : [], \"backrefs\": [], \"xml:space\": \"preserve\", \"force\": false, \"language\"\
  : \"python\", \"highlight_args\": {}, \"linenos\": false} -->\n\n````python\n\n\
  \   {\n       \"rouge_precision\": 1.0,\n       \"rouge_recall\": 1.0,\n       \"\
  rouge_f1_score\": 1.0\n   }\n   ````"
constructor:
  syntax: 'RougeScoreEvaluator(rouge_type: RougeType)'
  parameters:
  - name: rouge_type
    isRequired: true
